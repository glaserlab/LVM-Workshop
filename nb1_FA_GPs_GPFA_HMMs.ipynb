{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glaserlab/LVM-Workshop/blob/main/nb1_FA_GPs_GPFA_HMMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**"
      ],
      "metadata": {
        "id": "-iximm4Hj6Ah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEioygwHj1Qk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Some helper functions for Gaussians\n",
        "from scipy.stats import norm\n",
        "from numpy.random import normal, multivariate_normal\n",
        "\n",
        "#Sci-kit linear factor analysis and PCA\n",
        "from sklearn.decomposition import FactorAnalysis, PCA\n",
        "\n",
        "#Some other scikit learn models we'll be using\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Factor Analysis\n",
        "\n",
        "1) Write code to simulate data from the  factor analysis model with the following parameters, with 1 latent and 3-dimensional observations:  \n",
        "\n",
        "$W = \\begin{bmatrix} -1 \\\\ 1 \\\\ 2 \\end{bmatrix}$\n",
        "$\\Psi = \\begin{bmatrix} 10 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
        "\n",
        "<br>\n",
        "\n",
        "*Credit: This problem is inspired from a problem from Jonathan Pillow's Computational Neuroscience course at Princeton*"
      ],
      "metadata": {
        "id": "AeOZO4vVk-Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) First, generate 2000 samples of 1-dimensional latent $z$"
      ],
      "metadata": {
        "id": "uVL8qog9ljzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the random seed so we get the same results every time\n",
        "np.random.seed(1)\n",
        "\n",
        "#Fill in below"
      ],
      "metadata": {
        "id": "zPZd95oklgqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change below code if necessary, based on the variable name you gave to Z\n",
        "\n",
        "# Sort samples of Z in order to make visualization of the latent more obvious later on\n",
        "Z=np.sort(Z)"
      ],
      "metadata": {
        "id": "Hbi8Mez7JTPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) From the latent, generate 3-dimensional samples $x$. <br> Also, create a version, $x_{noiseless}$, that doesn't include the observation noise ($\\Psi$)."
      ],
      "metadata": {
        "id": "-i78o4OEleF2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rC3hWNmk5Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Make a scatter plot showing the first two dimensions of $x$ samples.\n",
        "<br> Overlay the same for $x_{noiseless}$, to get an intuition for how the shared signal and noise differ.\n",
        "<br> Make the x and y axes have equal limits"
      ],
      "metadata": {
        "id": "qn_Gp6MgnNKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter()\n",
        "plt.scatter()\n",
        "plt.ylim([-12,12])\n",
        "plt.xlim([-12,12])"
      ],
      "metadata": {
        "id": "c9vx2b9Jq5_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) To make this more clearly connected to neuroscience, let's assume each of these samples was taken over the course of 2000 time points. Plot the latent and the first two dimensions of $x$ (the 'activity of two neurons') by filling in the below code"
      ],
      "metadata": {
        "id": "qsPYPX6IsEZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot latent\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot()\n",
        "\n",
        "#Plot first dimension of x\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot()\n",
        "\n",
        "#Plot second dimension of x\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "tS5HUOISojZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e) Fit a factor analysis model with 1 latent to the data, using sci-kit learn. Note that the package has already been imported"
      ],
      "metadata": {
        "id": "8_v70B2quRX8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYYdnPd8uPlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f) Print the model's loadings (components_) and noise (noise_variance_) to check that they're approximately the same as the model you generated the data from."
      ],
      "metadata": {
        "id": "kDjHZVB0UYj_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4m560rlusFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPr3fOUgu38P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "g) Fit a PCA model to the data. You can also use sci-kit learn. Print the loadings (components_) to see how this differs from factor analysis"
      ],
      "metadata": {
        "id": "jVxuU2NKUswG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXYTQeV67X0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "h) Replot the scatter plot from (c). Now overlay a line with the FA loading axis and a line with the PCA loading axis (just the first two dimensions of those). Include a legend."
      ],
      "metadata": {
        "id": "fvU94DiNVcDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[0,:],X[1,:])\n",
        "plt.scatter(X_noiseless[0,:],X_noiseless[1,:])\n",
        "plt.plot([-10*pca.components_[0][0],10*pca.components_[0][0]],[-10*pca.components_[0][1],10*pca.components_[0][1]],'r--',linewidth=3)\n",
        "plt.plot([-10*fa.components_[0][0],10*fa.components_[0][0]],[-10*fa.components_[0][1],10*fa.components_[0][1]],'k--',linewidth=3)\n",
        "\n",
        "plt.ylim([-12,12])\n",
        "plt.xlim([-12,12])\n",
        "\n",
        "plt.legend(['X','X_noiseless','PCA axis','FA axis'])"
      ],
      "metadata": {
        "id": "e_dWg5vDZyAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Explain in words, in the cell below, why the loadings for PCA and FA are different"
      ],
      "metadata": {
        "id": "WovOe3K6KZ-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXBO5afxX6zD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "j) Plot the ground truth latent, and the recovered latent via FA, overlaid"
      ],
      "metadata": {
        "id": "V_L_FfQjhDBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot latent recovered from factor analysis\n",
        "plt.plot()\n",
        "\n",
        "#Plot ground truth latent\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "pBxCzavhhcJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k) Plot the ground truth latent and the recovered latent via PCA, overlaid"
      ],
      "metadata": {
        "id": "5UjbSpN9iNon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot latent recovered from PCA\n",
        "plt.plot()\n",
        "\n",
        "#Plot ground truth latent\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "9uVxvqwlhqwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L) Explain in words, in the cell below, why PCA's estimate of the latent are so inaccurate relative to FA. In particular, consider which dimensions are being most heavily utilized to estimate the latent.\n",
        "\n"
      ],
      "metadata": {
        "id": "qgMVOawKpY_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CTu1ovUWpf2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "m) While you were able to use sci-kit learn for factor analysis rather than fully implementing EM, let's just do the \"E\" step here using the final model parameters found with sci-kit learn. That is, find p(z|x), using the equation shown in class. You can just find the mean here and not worry about the variance."
      ],
      "metadata": {
        "id": "PmsDYJr8kbEN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cvhcwFeHpicS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a scatter plot of the latent estimated above, versus the latent directly output from sci-kit learn. If the above calculations were correct, this should be a diagonal line."
      ],
      "metadata": {
        "id": "zFYKpDa_M8H-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zczppAFqQJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n) The above showed pretty large differences between factor analysis and PCA to demonstrate how they differ, but for most neural datasets, the difference is more minor. <br><br> Re-run the above with FA vs PCA comparison on data generated with the following, less extreme, noise model:\n",
        "$\\Psi = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
        "<br>\n",
        "Just output the same plot as in (h)\n",
        "\n",
        "And by re-run, I mean copy-paste your code from a,b,e,g,h (while changing $\\Psi$) into your cell below, so you don't overwrite your previous results."
      ],
      "metadata": {
        "id": "6PXz26xTb5xk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cx6peOd7N5hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B21Bv2eSaUOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Gaussian Processes"
      ],
      "metadata": {
        "id": "PGL_KPi7uJY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have provided a function for a radial basis function kernel below"
      ],
      "metadata": {
        "id": "0l5WJxz4P1YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In the below function, x is a vector of datapoints, and L (the lengthscale) is a scalar\n",
        "def cov_RBF(x,L):\n",
        "  cov_rbf = np.exp(-(x-x.T)**2/(2*L**2))\n",
        "  return cov_rbf"
      ],
      "metadata": {
        "id": "bAnaO3Xu5tjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Let's say our datapoints have values 1,2,...,399,400 (e.g. these are the values of timepoints).  Plot (imshow) the RBF covariance for those values, for lengthscale=100 and lengthscale=1"
      ],
      "metadata": {
        "id": "amM1jcWRzMPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.arange(1,400)[None,:]\n",
        "\n",
        "scale1=\n",
        "K_rbf1 = cov_RBF()\n",
        "plt.figure()\n",
        "plt.imshow(K_rbf1,clim=[0,1])\n",
        "plt.title('Lengthscale 100')\n",
        "plt.colorbar()\n",
        "\n",
        "scale2=\n",
        "K_rbf2 = cov_RBF()\n",
        "plt.figure()\n",
        "plt.imshow(K_rbf2,clim=[0,1])\n",
        "plt.title('Lengthscale 1')\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "8jOKXsJX9Roa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) For both of the above RBF covariance functions, sample datapoints from the multivariate normal distribution with mean 0 and those covariances. Plot both of these samples. Note that the result will be 400 datapoints. As a hint, the means that you input into the multivariate_normal function will need to be arrays with 400 zeros."
      ],
      "metadata": {
        "id": "HPK4hEoI0Gu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu=\n",
        "\n",
        "sample1 = np.random.multivariate_normal(mean=, cov=)\n",
        "plt.figure()\n",
        "plt.plot(sample1)\n",
        "plt.title('GP Sample for lengthscale=100')\n",
        "\n",
        "sample2 = np.random.multivariate_normal(mean=, cov=)\n",
        "plt.figure()\n",
        "plt.plot(sample2)\n",
        "plt.title('GP Sample for lengthscale=1')"
      ],
      "metadata": {
        "id": "Gf8UfzwldIYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Explain in words how the differences in the covariance functions in (a) (for different scale parameters) explain the differences in the samples from the distributions (b)"
      ],
      "metadata": {
        "id": "lsduY-Uv1s9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ADI6d-MG2VzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Gaussian Process Factor Analysis (GPFA)"
      ],
      "metadata": {
        "id": "thCLPXMLV_5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no sections of this portion of the notebook for you to complete. Instead, we will demo how to run Gaussian Process Factor Analysis using the `elephant` package. Run through each cell to generate the plots."
      ],
      "metadata": {
        "id": "mo7uaVCk617D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install elephant"
      ],
      "metadata": {
        "id": "pimF8mEFWDi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### CREDIT: elephant documentation https://elephant.readthedocs.io/en/latest/tutorials/gpfa.html\n",
        "import numpy as np\n",
        "import quantities as pq\n",
        "import neo\n",
        "from elephant.spike_train_generation import inhomogeneous_poisson_process\n",
        "\n",
        "def integrated_oscillator(dt, num_steps, x0=0, y0=1, angular_frequency=2*np.pi*1e-3):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    dt : float\n",
        "        Integration time step in ms.\n",
        "    num_steps : int\n",
        "        Number of integration steps -> max_time = dt*(num_steps-1).\n",
        "    x0, y0 : float\n",
        "        Initial values in three dimensional space.\n",
        "    angular_frequency : float\n",
        "        Angular frequency in 1/ms.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    t : (num_steps) np.ndarray\n",
        "        Array of timepoints\n",
        "    (2, num_steps) np.ndarray\n",
        "        Integrated two-dimensional trajectory (x, y, z) of the harmonic oscillator\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(num_steps, int), \"num_steps has to be integer\"\n",
        "    t = dt*np.arange(num_steps)\n",
        "    x = x0*np.cos(angular_frequency*t) + y0*np.sin(angular_frequency*t)\n",
        "    y = -x0*np.sin(angular_frequency*t) + y0*np.cos(angular_frequency*t)\n",
        "    return t, np.array((x, y))\n",
        "\n",
        "def random_projection(data, embedding_dimension, loc=0, scale=None):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        Data to embed, shape=(M, N)\n",
        "    embedding_dimension : int\n",
        "        Embedding dimension, dimensionality of the space to project to.\n",
        "    loc : float or array_like of floats\n",
        "        Mean (“centre”) of the distribution.\n",
        "    scale : float or array_like of floats\n",
        "        Standard deviation (spread or “width”) of the distribution.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "       Random (normal) projection of input data, shape=(dim, N)\n",
        "\n",
        "    See Also\n",
        "    --------\n",
        "    np.random.normal()\n",
        "\n",
        "    \"\"\"\n",
        "    if scale is None:\n",
        "        scale = 1 / np.sqrt(data.shape[0])\n",
        "    projection_matrix = np.random.normal(loc, scale, (embedding_dimension, data.shape[0]))\n",
        "    return np.dot(projection_matrix, data)\n",
        "\n",
        "\n",
        "def generate_spiketrains(instantaneous_rates, num_trials, timestep):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    instantaneous_rates : np.ndarray\n",
        "        Array containing time series.\n",
        "    timestep :\n",
        "        Sample period.\n",
        "    num_steps : int\n",
        "        Number of timesteps -> max_time = timestep*(num_steps-1).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    spiketrains : list of neo.SpikeTrains\n",
        "        List containing spiketrains of inhomogeneous Poisson\n",
        "        processes based on given instantaneous rates.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    spiketrains = []\n",
        "    for _ in range(num_trials):\n",
        "        spiketrains_per_trial = []\n",
        "        for inst_rate in instantaneous_rates:\n",
        "            anasig_inst_rate = neo.AnalogSignal(inst_rate, sampling_rate=1/timestep, units=pq.Hz)\n",
        "            spiketrains_per_trial.append(inhomogeneous_poisson_process(anasig_inst_rate))\n",
        "        spiketrains.append(spiketrains_per_trial)\n",
        "\n",
        "    return spiketrains"
      ],
      "metadata": {
        "id": "4Bx8Ld-sd0xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulation parameters for generating ground-truth latents"
      ],
      "metadata": {
        "id": "XAruMLH0_tN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Parameters for oscillator used to generate ground-truth latents\n",
        "timestep = 1 * pq.ms        # Each timestep is 1 ms\n",
        "trial_duration = 2 * pq.s   # Trial duration is 2 ms\n",
        "num_steps = int((trial_duration.rescale('ms')/timestep).magnitude) # Total number of steps is 2000\n",
        "\n",
        "# specify data size\n",
        "num_trials = 20\n",
        "num_spiketrains = 50 # This is how many \"neurons\" we're inputting to GPFA"
      ],
      "metadata": {
        "id": "iF3f49CB9DDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate the ground-truth latents and visualize"
      ],
      "metadata": {
        "id": "AAv-xt_z_yrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# generate a low-dimensional trajectory\n",
        "times_oscillator, oscillator_trajectory_2dim = integrated_oscillator(\n",
        "    timestep.magnitude,\n",
        "    num_steps=num_steps,\n",
        "    x0=0,\n",
        "    y0=1\n",
        ")\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(oscillator_trajectory_2dim[0,:])\n",
        "plt.grid(ls=\":\")\n",
        "plt.ylabel(\"Latent #1\")\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(oscillator_trajectory_2dim[1,:], c='C1')\n",
        "plt.xlabel(\"timestep (ms)\"), plt.ylabel(\"Latent #2\")\n",
        "plt.grid(ls=\":\")"
      ],
      "metadata": {
        "id": "XlKg7wnG_1mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate high-dimensional \"neural\" trajectory"
      ],
      "metadata": {
        "id": "vcj5zZYpAR7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's set the average firing rate pretty high for the sake of simulation\n",
        "avg_fr = 50\n",
        "\n",
        "# random projection to high-dimensional space - resulting shape is neurons x timesteps\n",
        "oscillator_trajectory_Ndim = random_projection(\n",
        "    oscillator_trajectory_2dim,\n",
        "    embedding_dimension=num_spiketrains\n",
        ")\n",
        "\n",
        "# Normalize to instaneous rate\n",
        "normed_traj = oscillator_trajectory_Ndim / oscillator_trajectory_Ndim.max()\n",
        "\n",
        "# generate spike trains\n",
        "spiketrains_oscillator = generate_spiketrains(\n",
        "    (normed_traj-normed_traj.min()) * avg_fr,\n",
        "    num_trials,\n",
        "    timestep\n",
        ")"
      ],
      "metadata": {
        "id": "qdCN7JLUAWKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(spiketrains_oscillator))"
      ],
      "metadata": {
        "id": "IxViEVoyUXFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note on formatting inputs for GPFA\n",
        "* This is important if you're interested in running GPFA on your own data.\n",
        "* List of trials, where each trial is a list with the following format:\n",
        "    [`neo.SpikeTrain` for neuron #1, ...]\n",
        "* Length of outer list should be `num_trials`, length of inner list should be `num_neurons`\n",
        "* Use `neo.SpikeTrain(times=[3, 4, 5], units='sec', t_stop=10.0)`, replacing times with your spike times to make the proper conversion for `neo`.\n",
        "* Check types on `spiketrains_oscillator` ^ for an example on the proper format."
      ],
      "metadata": {
        "id": "qJ_HoI2JU-N4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now fit GPFA"
      ],
      "metadata": {
        "id": "bpq1ALvABxm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elephant.gpfa import GPFA\n",
        "gpfa_2dim = GPFA(bin_size=10*pq.ms, x_dim=2)\n",
        "\n",
        "\n",
        "### Input must be a list of trials, where each trial is a list of neo.SpikeTrains objects\n",
        "gpfa_2dim.fit(spiketrains_oscillator) # Input must be a neo.SpikeTrains object"
      ],
      "metadata": {
        "id": "jhQtVN3JBxF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Infer the latents for GPFA\n",
        "We will visualize the inferred latents from GPFA alongside latents inferred using pre-smoothing and factor analysis below."
      ],
      "metadata": {
        "id": "_fvBcP9sT3V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trajectories = gpfa_2dim.transform(spiketrains_oscillator)"
      ],
      "metadata": {
        "id": "7C9p_LldEZgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert spike times to binned spike trains\n",
        "* We are going to bin our spikes at 10 ms and apply a fixed level of pre-smoothing for each latent dimension.\n",
        "* Note that the $\\sigma$ for the Gaussian kernel used for pre-smoothing will be in `bins` not ms."
      ],
      "metadata": {
        "id": "JknGHryPSMiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from elephant.conversion import BinnedSpikeTrain\n",
        "\n",
        "# For fitting factor analysis, let's convert SpikeTrains to binned spike trains\n",
        "X = [BinnedSpikeTrain(s, bin_size=10*pq.ms).to_array().T for s in spiketrains_oscillator]"
      ],
      "metadata": {
        "id": "p99_3A5M3mNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's apply some pre-smoothing before applying factor analysis"
      ],
      "metadata": {
        "id": "QGsULy4aTyAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "# Using 25 ms Gaussian kernel\n",
        "X_sm = [gaussian_filter1d(x.astype('float32'), sigma=2.5, axis=0) for x in X]"
      ],
      "metadata": {
        "id": "iePhCldjSKyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's apply factor analysis and compare results with ground-truth and GPFA"
      ],
      "metadata": {
        "id": "nF2_aj_uT8Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import FactorAnalysis\n",
        "fa = FactorAnalysis(n_components=2).fit(np.concatenate(X_sm, axis=0))\n",
        "latents_fa = [fa.transform(x) for x in X_sm]\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10,5))\n",
        "\n",
        "# Plot the latents extracted using presmoothing + factor analysis\n",
        "axs[0].plot(latents_fa[0][:,0])\n",
        "axs[0].plot(latents_fa[0][:,1])\n",
        "axs[0].set_ylabel(\"Presmoothing & \\n factor analysis latent\")\n",
        "axs[0].set_ylim(-2,2)\n",
        "\n",
        "# Plot the latents extracted using GPFA (learned-smoothing)\n",
        "axs[1].plot(trajectories[0].T[:,0])\n",
        "axs[1].plot(trajectories[0].T[:,1])\n",
        "axs[1].set_ylabel(\"GPFA latent\")\n",
        "axs[1].set_ylim(-2,2)\n",
        "\n",
        "# Plot the ground-truth latents\n",
        "axs[2].plot(oscillator_trajectory_2dim[0,:])\n",
        "axs[2].plot(oscillator_trajectory_2dim[1,:])\n",
        "axs[2].set_ylabel(\"Ground-truth latent\")\n",
        "axs[2].set_ylim(-2,2)\n"
      ],
      "metadata": {
        "id": "AdPL9AK9UJFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Models"
      ],
      "metadata": {
        "id": "0DHFkZtNa7WY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A) Write code to simulate data from the an HMM with 2 discrete states (indexed by 0 and 1), with 1-dimensional Gaussian observations.\n",
        "<br><br>\n",
        "The initial state probabilities are:<br>\n",
        "$P(z_0=0)=0.5$,\n",
        "$P(z_0=1)=0.5$\n",
        "<br><br>\n",
        "The transitions matrix is:<br>\n",
        "$A = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.05 & 0.95 \\end{bmatrix}$\n",
        "<br><br>\n",
        "The emissions probabilities are:<br>\n",
        "$P(y|z=0)=N(0,1)$\n",
        "<br>\n",
        "$P(y|z=1)=N(2,1)$\n",
        "\n",
        "<br><br>\n",
        "We've provided code below that you can fill in (which should hopefully make this faster for you)\n",
        "\n",
        "*Credit: Note that the below code has been adapted from Neuromatch's HMM exercise.*"
      ],
      "metadata": {
        "id": "ghq4nXWia_Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial probabilities\n",
        "initial_probs=\n",
        "\n",
        "#Transition matrix\n",
        "transition_matrix=\n",
        "\n",
        "#Means for the 2 states\n",
        "means=\n",
        "\n",
        "#Variances for the 2 states\n",
        "vars=\n",
        "\n",
        "#Number of time points to run the simulation for\n",
        "T=500\n",
        "\n",
        "#We'll set the random seed so results are reproducible\n",
        "np.random.seed(0)\n",
        "\n",
        "# Initialize the latent (Z) and observation (X)\n",
        "Z = np.zeros((T,),dtype=int)\n",
        "X = np.zeros((T,))\n",
        "\n",
        "# Sample initial (time 0) latent state\n",
        "Z[0] = np.random.choice([0,1],p=    )\n",
        "\n",
        "# Given the latent that was just sampled, determine the observation at time 0\n",
        "X[0] = normal()\n",
        "\n",
        "# Loop over time points\n",
        "# At each time point, sample the next latent state (based on the transition matrix and the previous state),\n",
        "# and then determine the observation at that time point\n",
        "for t in range(1,T):\n",
        "\n",
        "  # Determine latent state at time `t`\n",
        "  transition_vector =\n",
        "  Z[t] = np.random.choice([0,1],p=transition_vector)\n",
        "\n",
        "  #Given the latent that was just sampled, determine the observation at time t\n",
        "  X[t] = normal()"
      ],
      "metadata": {
        "id": "0bkroE10a9C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to plot the latent (Z) and the observations (X)"
      ],
      "metadata": {
        "id": "rOGHzUG6bxLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(Z[None,:],aspect='auto')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(X)"
      ],
      "metadata": {
        "id": "S4MwkrBFbyD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Fit K-means to the observations. You can use sci-kit learn (imported above)"
      ],
      "metadata": {
        "id": "XeX4Zi3-b9qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8nRvJUvbzr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below code to see how the states learned compare to the ground truth HMM states. Change \"Z_kmeans\" to the variable name of the kmeans states you used above."
      ],
      "metadata": {
        "id": "d6pKdJMRcD6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(Z[None,:],aspect='auto')\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.imshow(Z_kmeans[None,:],aspect='auto')"
      ],
      "metadata": {
        "id": "_3QHjxQ0cEma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C)Calculate the accuracy of K-means at finding the ground truth HMM states"
      ],
      "metadata": {
        "id": "0jaIbK7scJ_d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oqut9JJhcGV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) We are not going to have you code up the full EM algorithm, below is a didactic implementation of it you should run through to get a better understanding of the optimization.\n",
        "\n",
        "The first part will find the $\\alpha$s for all time points."
      ],
      "metadata": {
        "id": "AK_kblM0cQlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Forward algorithm\n",
        "K=2\n",
        "\n",
        "alphas=np.zeros((T,K))\n",
        "\n",
        "t=0\n",
        "\n",
        "emission_probs=np.zeros(K)\n",
        "for k in range(K):\n",
        "  emission_probs[k]=norm.pdf(X[t],means[k],vars[k])\n",
        "\n",
        "alphas_tmp=np.array(initial_probs)*emission_probs\n",
        "alphas[t,:]=alphas_tmp/np.sum(alphas_tmp)\n",
        "\n",
        "for t in range(1,T):\n",
        "  emission_probs=np.zeros(K)\n",
        "  for k in range(K):\n",
        "    emission_probs[k]=norm.pdf(X[t],means[k],vars[k])\n",
        "\n",
        "  alphas_tmp=(transition_matrix.T@alphas[t-1])*emission_probs\n",
        "  alphas[t,:]=alphas_tmp/np.sum(alphas_tmp)"
      ],
      "metadata": {
        "id": "Sx5ooAQ0cMOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below code to see how the alphas compare to the ground truth HMM states. Change \"alphas\" to the variable name you used above. This is a good sanity check that the above code is correct, since the alphas for one of the states will look somewhat similar to the ground truth (although the values won't always be exactly 0 and 1)."
      ],
      "metadata": {
        "id": "LcPF8sypcWKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(alphas)\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(Z)"
      ],
      "metadata": {
        "id": "MmKTFTpGcOoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) Below is an implementation of the backward algorithm for HMMs. Use it to calculate \"betas\" for all time points."
      ],
      "metadata": {
        "id": "pbcF8SP0ccgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Backward algorithm\n",
        "K=2\n",
        "\n",
        "betas=np.zeros((T,K))\n",
        "\n",
        "t=T-1\n",
        "\n",
        "betas_tmp=np.array([1,1])\n",
        "betas[t,:]=betas_tmp/np.sum(betas_tmp)\n",
        "\n",
        "for t in range(T-2,-1,-1):\n",
        "  emission_probs=np.zeros(K)\n",
        "  for k in range(K):\n",
        "    emission_probs[k]=norm.pdf(X[t+1],means[k],vars[k])\n",
        "\n",
        "  betas_tmp=transition_matrix@(betas[t+1]*emission_probs)\n",
        "\n",
        "  betas[t,:]=betas_tmp/np.sum(betas_tmp)"
      ],
      "metadata": {
        "id": "7Doi6dAWcY8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below code to see how the betas compare to the ground truth HMM states. This is a good sanity check that the above code is correct, since the betas for one of the states will look somewhat similar to the ground truth (although the values won't always be exactly 0 and 1)."
      ],
      "metadata": {
        "id": "_3W4E28UcgRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(betas)\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(Z)"
      ],
      "metadata": {
        "id": "41itwBuEceNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F) Run the forward/backward algorithm (combining the steps above) to get the posterior, p(z|x)."
      ],
      "metadata": {
        "id": "ytOeFCK0clIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine forward/backward to  get posterior\n",
        "\n",
        "posterior=np.zeros((T,K))\n",
        "for t in range(T):\n",
        "\n",
        "  posterior_tmp=alphas[t]*betas[t]\n",
        "\n",
        "  posterior[t]=posterior_tmp/np.sum(posterior_tmp)"
      ],
      "metadata": {
        "id": "A_Pip5MkciNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below code to see how the posterior compares to the ground truth HMM states. Change \"posterior\" to the variable name you used above."
      ],
      "metadata": {
        "id": "fqc3awJ_cp2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(posterior)\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(Z)"
      ],
      "metadata": {
        "id": "wsj1wUhGcnHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "G) Calculate the accuracy of the posterior at estimating the ground truth latent states. Set a probability threshold of 0.5 to determine which of the two states is more likely."
      ],
      "metadata": {
        "id": "hoffRRzocuYV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UCMpKpM9csR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "H) In words, in the text box below, describe why the HMM led to more accurate results than fitting with K-means.\n",
        "\n",
        "As a side note, this increase in accuracy is not just because we were cheating above and using the ground truth parameters. Fitting an HMM with EM gives similar results."
      ],
      "metadata": {
        "id": "GRD4wj7McyL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FxA3Xoazc2Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some final notes\n",
        "There are more advanced versions of HMMs, where in addition to time-varying disecrete latent states, you have state-dependent model parameters. One example is the GLM-HMM, where each discrete state additionally corresponds to a set of weights on a generalized linear model.\n",
        "\n",
        "Link to [GLM-HMM tutorial](https://github.com/zashwood/ssm/blob/master/notebooks/2b%20Input%20Driven%20Observations%20(GLM-HMM).ipynb)"
      ],
      "metadata": {
        "id": "QPIIqvRUfMiP"
      }
    }
  ]
}