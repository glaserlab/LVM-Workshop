{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glaserlab/LVM-Workshop/blob/main/nb2_LDS_PLDS_SLDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**"
      ],
      "metadata": {
        "id": "-iximm4Hj6Ah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEioygwHj1Qk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Some helper functions for Gaussians\n",
        "from numpy.random import normal, multivariate_normal\n",
        "from scipy.stats import norm\n",
        "\n",
        "#Some scikit learn models we'll be using\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA,FactorAnalysis\n",
        "\n",
        "#To load the data file\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Linear dynamical systems (Kalman filters)"
      ],
      "metadata": {
        "id": "PGL_KPi7uJY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using the ssm package for fitting linear dynamical systems. Run the code below to install that package."
      ],
      "metadata": {
        "id": "0DWTCICafX0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/lindermanlab/ssm.git@master#egg=ssm\n",
        "\n",
        "import ssm"
      ],
      "metadata": {
        "id": "kVBXJCXIpjqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Fill in the code below to simulate data from a linear dynamical system model with 2-dimensional latents, and 5-dimensional observations.\n",
        "\n",
        "<br>\n",
        "The dynamics matrix is the scaled rotation matrix (which will cause the latents to spiral) :<br>\n",
        "$A = \\begin{bmatrix} cos(\\theta) & -sin(\\theta) \\\\ sin(\\theta) & cos(\\theta) \\end{bmatrix} \\text{for  } \\theta=\\pi/20$\n",
        "\n",
        "<br><br>\n",
        "In the code below, the emissions (observations) matrix is random, and there is more noise in the observations than the dynamics (see code below).\n"
      ],
      "metadata": {
        "id": "vpo3EIbyUV3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "#Dimensionality of latents\n",
        "K=2\n",
        "#Dimensionality of observations\n",
        "N=5\n",
        "\n",
        "#Initial latent state\n",
        "Z0=np.array([.5,.5])\n",
        "\n",
        "#Dynamics matrix\n",
        "th=np.pi/20\n",
        "A = .995 * np.array([[np.cos(th),-np.sin(th)],[np.sin(th),np.cos(th)]])\n",
        "\n",
        "#Noise of dynamics\n",
        "Sig_dynamics=.0001*np.identity(K)\n",
        "\n",
        "#Emissions (observations) matrix\n",
        "C=np.random.randn(K,N)\n",
        "\n",
        "#Noise of observations\n",
        "Sig_observations=.01*np.identity(N)\n",
        "\n",
        "#Number of time points to simulate for\n",
        "T=500\n",
        "\n",
        "#Initialize latents and observations\n",
        "Z = np.zeros((T,K))\n",
        "Y = np.zeros((T,N))\n",
        "\n",
        "#Set initial state (at time 0) of the latents\n",
        "Z[0]=Z0\n",
        "\n",
        "#Update observations at time 0 according to the latent state\n",
        "Y[0]=\n",
        "\n",
        "# Loop over time points\n",
        "# At each time point, sample the next latent state (based on the dynamics matrix and the previous state),\n",
        "# and then determine the observation at that time point\n",
        "for t in range(1,T):\n",
        "  #Update latent state at time t according to dynamics\n",
        "  Z[t]=\n",
        "\n",
        "  #Update observations at time t according to the latent state\n",
        "  Y[t]="
      ],
      "metadata": {
        "id": "eD_hSVObfl1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to plot the two dimensions of the latent against each other (to see the spiral if the above simulation is correct)"
      ],
      "metadata": {
        "id": "5o94ChP1cI2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Z[:,0],Z[:,1])"
      ],
      "metadata": {
        "id": "JTh5Jw6ghy0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to plot the two dimensions of the latent as a function of time"
      ],
      "metadata": {
        "id": "X_8VcVLtccEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Z[:,0])\n",
        "plt.plot(Z[:,1])\n",
        "plt.legend(['Latent Dim 1','Latent Dim 2'])"
      ],
      "metadata": {
        "id": "en0ix02xc5NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to plot the first two dimensions of the observations as a function of time"
      ],
      "metadata": {
        "id": "9qwDCZbDczcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Y[:,0])\n",
        "plt.plot(Y[:,1])\n",
        "plt.legend(['Obs Dim 1','Obs Dim 2'])"
      ],
      "metadata": {
        "id": "GoLz3gvUZ8Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Fit a factor analysis model (using sci-kit learn) with 2 latents to the above observations"
      ],
      "metadata": {
        "id": "P7Hgvuo4dEcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fa=FactorAnalysis(2)\n",
        "z_fa=fa.fit_transform(Y)"
      ],
      "metadata": {
        "id": "r9_eEzMlb8yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Plot the 2 factor analysis latents as a function of time"
      ],
      "metadata": {
        "id": "UtoBB8KLhmQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(z_fa)"
      ],
      "metadata": {
        "id": "MnbH4whqb806"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Fit an LDS model to the data\n",
        "\n",
        "You can run the ssm code below (no need to make any changes). Still, read through the code/comments to try to generally understand the below code."
      ],
      "metadata": {
        "id": "jAxkv1t3I_Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs_dim=N\n",
        "latent_dim=K\n",
        "\n",
        "#Define an LDS model that has a Gaussian emissions model, with the correct latent and observation dimensions\n",
        "lds = ssm.LDS(obs_dim, latent_dim, emissions=\"gaussian\")\n",
        "\n",
        "#Fit the LDS model using EM\n",
        "#Note the method says 'laplace_em', which becomes standard EM with Gaussian emissions\n",
        "elbos, q = lds.fit(Y, method=\"laplace_em\", num_iters=10)\n",
        "\n",
        "#The output q contains the posterior distribution (the latent's mean and variance)\n",
        "#Below, we'll just extract the mean of the posterior at each time point (the value we usually think of as the latent):\n",
        "z_lds = q.mean_continuous_states[0]"
      ],
      "metadata": {
        "id": "CtGFHN6dcQH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) Plot the latents learned by LDS"
      ],
      "metadata": {
        "id": "7z4JdsfJMxdi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UYnoz6fVcQK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F) How do the learned latents differ between the factor analysis and LDS models? Why?  (Enter your answer in the text box below)"
      ],
      "metadata": {
        "id": "FIY9zGpdhwQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d8rh9Nl9hxTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "G) Run the below code to check that the eigenvalues of the system's dynamics (the A matrix) were recovered correctly. Note that the eigenvectors, and the A matrices themselves won't be identical, because the emissions matrix can be recovered differently from the ground truth (e.g. some linear combination of the ground truth)."
      ],
      "metadata": {
        "id": "Erpgae8-AIvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eigendecomposition of true dynamics\n",
        "np.linalg.eig(A)"
      ],
      "metadata": {
        "id": "JQXmYVMOB1vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eigendecomposition of learned dynamics\n",
        "np.linalg.eig(lds.dynamics.As)"
      ],
      "metadata": {
        "id": "o_tKIzdIB8eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Linear Dynamical Systems on Neural Data\n",
        "\n"
      ],
      "metadata": {
        "id": "7tnM22RuiJzl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDYHQuPkiB69"
      },
      "source": [
        "Now we're going to fit an LDS to some real neural data. This is data collected in Lee Miller's lab from motor cortex while a monkey continues to reach to targets across a workspace.\n",
        "Within a trial, a monkey typically makes 4 reaches (although occasionally less if he makes an error and the trial ends early). After the 4 reaches there is a brief pause while the monkey gets a juice reward. All trials are concatenated together in time in the data. <br>\n",
        "\n",
        "\"Neural data\" is a variable of size Timepoints x Neurons, where each entry is the firing rate of a given neuron in that time bin. <br>\n",
        "\"Velocity\" is a variable of size Timepoints x 2, where each timepoint has the x and y velocities of the hand.\n",
        "\n",
        "Each time bin, of both neural activity and velocity, is duration 50ms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh-MwjXRcQN_"
      },
      "outputs": [],
      "source": [
        "#Download and load the data\n",
        "\n",
        "!wget -nc https://www.dropbox.com/s/jcief15oql3tkll/example_data_m1.pickle?dl=0\n",
        "\n",
        "filename    = 'example_data_m1.pickle?dl=0'\n",
        "with open(filename, 'rb') as handle:\n",
        "    [Neural_data_tmp,Velocity_tmp] = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're just going to extract the first 1000 time points for this assignment, so that the models fit faster."
      ],
      "metadata": {
        "id": "8zZKRceDiWbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Neural_data=Neural_data_tmp[:1000,:]\n",
        "Velocity=Velocity_tmp[:1000,:]"
      ],
      "metadata": {
        "id": "79wB5gm1iTS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Fit a Factor Analysis model to the Neural data with 10 latents"
      ],
      "metadata": {
        "id": "XH_suG9Dibuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_latents_fa="
      ],
      "metadata": {
        "id": "P5qx8V5LiYav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Fit a linear dynamical systems model to the neural data with 10 latents. Note that this will take around 30+ seconds to fit. You can just run the code below."
      ],
      "metadata": {
        "id": "nnVW3R-1ig2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "obs_dim=Neural_data.shape[1]\n",
        "latent_dim=10\n",
        "\n",
        "#Declare the LDS model\n",
        "lds = ssm.LDS(obs_dim, latent_dim, emissions=\"gaussian\")\n",
        "\n",
        "#Fit the LDS model\n",
        "elbos, q = lds.fit(Neural_data, method=\"laplace_em\", num_iters=10)\n",
        "\n",
        "#Get the latents:\n",
        "continuous_latents_lds = q.mean_continuous_states[0]"
      ],
      "metadata": {
        "id": "UruDGFMIieCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Run the code below to plot the first two FA and LDS latents for a brief snippet of time (200 time bins = 10 sec)"
      ],
      "metadata": {
        "id": "6E4ZRpPQil72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(continuous_latents_fa[:200,:3])\n",
        "plt.title('FA latents')\n",
        "plt.xticks([])\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(continuous_latents_lds[:200,:3])\n",
        "plt.title('LDS latents')\n"
      ],
      "metadata": {
        "id": "-D-Nbw24ii3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) At least to me, unlike the simulation example above, it's not obvious in the above plots that the LDS model has smoother latents. In the text box below, why do you think LDS doesn't lead to such smooth responses here. (Hint: There are realistically multiple reasons, but I'm mainly looking for whether LDS is a good model for this data, which contains an entire experiment concatenated together)."
      ],
      "metadata": {
        "id": "2zK3HkuEirNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Rv7IWw-Aits9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) Still, let's quantitatively see how 'relatively smooth' the latents are. For each latent, we'll calculate the squared change in the latent at each time point, relative to the average squared magnitude of the latent. A smaller value will mean it's smoother (changing less from one time point to the next). We will then plot a histogram of this relative change. Run the below code cells"
      ],
      "metadata": {
        "id": "2UjJ2OZdiw69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z=np.copy(continuous_latents_fa)\n",
        "relative_change_fa=[np.mean((z[1:,l]-z[:-1,l])**2)/np.mean(z[:,l]**2) for l in range(latent_dim)]\n",
        "print(relative_change_fa)"
      ],
      "metadata": {
        "id": "5OW7vtEhinl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z=np.copy(continuous_latents_lds)\n",
        "relative_change_lds=[np.mean((z[1:,l]-z[:-1,l])**2)/np.mean(z[:,l]**2) for l in range(latent_dim)]\n",
        "print(relative_change_lds)"
      ],
      "metadata": {
        "id": "UpnJee1ViyvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(relative_change_fa,alpha=0.5)\n",
        "plt.hist(relative_change_lds,alpha=0.5)\n",
        "plt.xlabel('Relative change (lower is smoother)')\n",
        "plt.ylabel('Number of latents')\n",
        "\n",
        "plt.legend(['FA','LDS'])"
      ],
      "metadata": {
        "id": "jz3lGVNJi0Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F) Does LDS lead to smoother latents (Yes or No)? Answer in the text box below."
      ],
      "metadata": {
        "id": "DforhHmbi6n2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KNX8iTLIi-ok"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp_dqVOKkVLL"
      },
      "source": [
        "G) One way to validate that we are getting a better estimate of the underlying latents is to try to see which set of latents best relates to a separate, external variable (here, velocity).\n",
        "\n",
        "Fit decoding models from 1) the Factor analysis latents to velocity, and 2) the LDS latents to velocity, and see which leads to better performance.\n",
        "\n",
        "In the decoding models, use latents that precede the velocity by 2 time bins (100 ms). That is, use Latents[:-2] to predict Velocity[2:]. This will improve decoding performance, since there is a lag for motor cortical signals to be maximally predictive of movement velocity.\n",
        "\n",
        "For the sake of time, you can do this is a non-rigorous (non-cross-validated way). Just report the \"score\" (R2 value) from two sci-kit learn linear regression models, where you train on all of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9y8Kooqt0t0"
      },
      "outputs": [],
      "source": [
        "#Fit linear regression model based on factor analysis latents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit linear regression model based on LDS latents"
      ],
      "metadata": {
        "id": "TWvH07cOjG7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PLDS Models"
      ],
      "metadata": {
        "id": "NrQJI9jUjTNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "G) Now we'll fit a Poisson Linear Dynamical system to the data. You can just run the code below."
      ],
      "metadata": {
        "id": "dcDFHyCVjVtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "obs_dim=Neural_data.shape[1]\n",
        "latent_dim=10\n",
        "\n",
        "#Declare the PLDS model\n",
        "lds = ssm.LDS(obs_dim, latent_dim, emissions=\"poisson\", emission_kwargs=dict(link=\"softplus\"))\n",
        "\n",
        "#Fit the PLDS model\n",
        "elbos, q = lds.fit(Neural_data.astype(int), method=\"laplace_em\", num_iters=10)\n",
        "\n",
        "#Get the latents:\n",
        "continuous_latents_plds = q.mean_continuous_states[0]"
      ],
      "metadata": {
        "id": "Wsf34yBvjIMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the ELBO over model iterations to check that the model has converged. You can just run the code below."
      ],
      "metadata": {
        "id": "fmzqUzUTjaER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(elbos)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('ELBO')"
      ],
      "metadata": {
        "id": "mqyK7b15jXnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mVcODnfgLZ7"
      },
      "source": [
        "H) Decode velocity based on the PLDS latents (again, with a lag of 2 time bins), in order to see how this compares to the LDS latents. Report the \"score\" (R2 value). Note that the value will be approximately the same here as what you got for LDS - when fitting the LDS/PLDS model with many more data points, a small difference starts to emerge. Note that a likely reason for the limited difference is the large time bins (50ms), and when there are many spikes within a time bin, a Gaussian approximation to a Poisson distribution is reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daRddlzcPQlt"
      },
      "outputs": [],
      "source": [
        "#Fit linear regression model based on PLDS latents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I) Plot the first 3 latents found with PLDS relative to LDS. To do so, fill in the code below."
      ],
      "metadata": {
        "id": "jfI1u1SQjnEA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wSAsBF2Ljnbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUeZ6DrKgZW2"
      },
      "source": [
        "J) Quantitatively determine how the relative smoothness of the latents found with PLDS compare to that of LDS. Create a histogram of the \"relative change\" of each latent for LDS and PLDS (you can mostly copy the code from above when you compared FA and LDS)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M-GqKgUjjo7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2ta87gXjvUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCSxeX2MPOSp"
      },
      "source": [
        "### SLDS models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K) Run an SLDS model with 2 discrete states, with a Poisson emissions model. You can just run the code below."
      ],
      "metadata": {
        "id": "V6Fh-nSJj2fi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z60JDQnlS1O2"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "obs_dim=Neural_data.shape[1]\n",
        "discrete_dim=2\n",
        "latent_dim=10\n",
        "\n",
        "#Declare the SLDS model\n",
        "slds2 = ssm.SLDS(obs_dim, discrete_dim, latent_dim, emissions=\"poisson\", emission_kwargs=dict(link=\"softplus\"))\n",
        "\n",
        "#Fit the SLDS model\n",
        "slds2_elbos, q_lem = slds2.fit(Neural_data.astype(int), method=\"laplace_em\",\n",
        "                               variational_posterior=\"structured_meanfield\",\n",
        "                               num_iters=20, alpha=0.0)\n",
        "#Get the latents:\n",
        "continuous_latents_slds = q_lem.mean_continuous_states[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L) Plot the ELBO over iterations, so you can get an idea that the model has converged. You can just run the code below."
      ],
      "metadata": {
        "id": "8N26fpDGj8dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(slds2_elbos)\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('ELBO')"
      ],
      "metadata": {
        "id": "tId-Efbqj5P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(1,20.1),slds2_elbos[1:])\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('ELBO')"
      ],
      "metadata": {
        "id": "4UPgjI0FkF_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2NJhCE2kKJP"
      },
      "source": [
        "M) Decode velocity based on the SLDS latents, in order to see how this compares to the previous LDS/PLDS latents (again with a lag of 2 time bins). Report the \"score\" (R2 value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Rz2wrGAclOg"
      },
      "outputs": [],
      "source": [
        "#Fit linear regression model based on SLDS latents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqNPp7ugQuvp"
      },
      "source": [
        "N) Run the code below to get the most likely states of the SLDS model, and plot them. We will plot the states over the behavioral velocity traces."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discrete_latents_slds = slds2.most_likely_states(continuous_latents_slds, Neural_data.astype(int))"
      ],
      "metadata": {
        "id": "65GRopO2kY5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(discrete_latents_slds[None,:1000],aspect='auto')\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(Velocity[:1000])\n",
        "plt.xlim([0,1000])"
      ],
      "metadata": {
        "id": "8v5eft8OkZUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O) In the text box below, write what behaviors the discrete states seem to belong to."
      ],
      "metadata": {
        "id": "qJQpfBFUkcjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9qQHpe3KkgOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "P) Fit an SLDS model with 3 discrete states."
      ],
      "metadata": {
        "id": "jidWiAWekjwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MdK86O9yhgD"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "obs_dim=Neural_data.shape[1]\n",
        "discrete_dim=3\n",
        "latent_dim=10\n",
        "\n",
        "#Declare the LDS model\n",
        "slds3 = ssm.SLDS(obs_dim, discrete_dim, latent_dim, emissions=\"poisson\", emission_kwargs=dict(link=\"softplus\"))\n",
        "\n",
        "#Fit the LDS model\n",
        "slds3_elbos, q_lem = slds3.fit(Neural_data.astype(int), method=\"laplace_em\",\n",
        "                               variational_posterior=\"structured_meanfield\",\n",
        "                               num_iters=20, alpha=0.0)\n",
        "#Get the latents:\n",
        "continuous_latents_slds = q_lem.mean_continuous_states[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q) Plot the most likely discrete states on top of the velocity traces, as before."
      ],
      "metadata": {
        "id": "W-hEucZbkoxY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iOa5GHsEkmOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnLjDJ3Ukqjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R) Is it clear what the 3 states relate to? Enter in the text box below."
      ],
      "metadata": {
        "id": "zH9seKhWk6s0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cV2n0yt9k9EL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfSF7_PISom2"
      },
      "source": [
        "S) Which SLDS model (the one with 2 or 3 discrete states) had a better fit to the training data, in terms of their ELBO. Print out the elbo value on the final iteration for the two models (this will be the last entry in 'slds2_elbos', for example), and then in the text box, state which is higher."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VNO2g_3NlBOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lDiYvlfqejSX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5e-CvwgSEdd"
      },
      "source": [
        "T) In the code below, we'll define a held-out test set as the next 1000 time points in the file. We will get the elbo on the held out data in the code below, which approximates the posterior (the latents) of the held out data, based on the already-fit-model's parameters, and then determines the elbo based on these test-set latents. After running the below code, print out the elbo value on the final iteration for the two models on the test set (e.g. slds2_test_elbos[-1]), and in the text box write which model is better."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Neural_data_test=Neural_data_tmp[1000:2000]"
      ],
      "metadata": {
        "id": "aticeMawlMND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slds2_test_elbos, posterior = slds2.approximate_posterior(Neural_data_test.astype(int),\n",
        "                                              method=\"laplace_em\",\n",
        "                                              variational_posterior=\"structured_meanfield\",\n",
        "                                              num_iters=20)"
      ],
      "metadata": {
        "id": "y9jZqRkolMbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slds3_test_elbos, posterior = slds3.approximate_posterior(Neural_data_test.astype(int),\n",
        "                                              method=\"laplace_em\",\n",
        "                                              variational_posterior=\"structured_meanfield\",\n",
        "                                              num_iters=20)"
      ],
      "metadata": {
        "id": "CpKGQV_JlN5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X9D0gmTflQrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "U) Finally, it is important to note that based on different random initializations, SLDS models can lead to differing results, and it's therefore valuable to test multiple models. Below is an SLDS model with 2 states with a different random seeds. It leads to a substantially worse training elbo, and qualitatively less sensible discrete states. Just run the code below to see this for yourselves."
      ],
      "metadata": {
        "id": "4tzMuwYnf4q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "obs_dim=Neural_data.shape[1]\n",
        "discrete_dim=2\n",
        "latent_dim=10\n",
        "\n",
        "#Declare the SLDS model\n",
        "slds2_v2 = ssm.SLDS(obs_dim, discrete_dim, latent_dim, emissions=\"poisson\", emission_kwargs=dict(link=\"softplus\"))\n",
        "\n",
        "#Fit the SLDS model\n",
        "slds2_v2_elbos, q_lem = slds2.fit(Neural_data.astype(int), method=\"laplace_em\",\n",
        "                               variational_posterior=\"structured_meanfield\",\n",
        "                               num_iters=20, alpha=0.0)\n",
        "#Get the continuous latents:\n",
        "continuous_latents_slds = q_lem.mean_continuous_states[0]\n",
        "\n",
        "#Get the discrete latents\n",
        "discrete_latents_slds = slds2_v2.most_likely_states(continuous_latents_slds, Neural_data.astype(int))"
      ],
      "metadata": {
        "id": "Z9EDsz_eldZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Previous Model 2 state training elbo:', slds2_elbos[-1])\n",
        "print('New Model 2 state training elbo:', slds2_v2_elbos[-1])"
      ],
      "metadata": {
        "id": "Xt8u70uclfWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,3))\n",
        "plt.subplot(2,1,1)\n",
        "plt.imshow(discrete_latents_slds[None,:1000],aspect='auto')\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(Velocity[:1000])\n",
        "plt.xlim([0,1000])"
      ],
      "metadata": {
        "id": "S-cV2d0tldsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}